{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Intro to Kubernetes Welcome to this introductory course to Kubernetes! Audience If you never operated a cluster, not even as a user, this course is for you. If you had previous exposure to the matter through webinars, blog posts, demos, this course might still be not too boring but if you know the difference between a Pod and a container, a Deployment and a Service, or you know what an Ingress does, well you might skip it. Prerequisites Despite being a 101 course, a solid understanding of Docker fundamentals (containers, images, registry, etc.) is required. Setup Before starting, please install Homebrew and follow these instructions in order to setup your laptop. Some system-wide binaries will be installed with brew while we are going to use asdf-vm for some specific programs like kubectl in order to have a better control over installes versions: Install system-wide required binaries by running brew bundle Install the following asdf-vm plugins by running: asdf plugin-add kubectl asdf plugin-add helm Finally, install versioned dependencies with asdf install kubectl create course You can now move to the first batch of exercises to start the course.","title":"Home"},{"location":"#intro-to-kubernetes","text":"Welcome to this introductory course to Kubernetes!","title":"Intro to Kubernetes"},{"location":"#audience","text":"If you never operated a cluster, not even as a user, this course is for you. If you had previous exposure to the matter through webinars, blog posts, demos, this course might still be not too boring but if you know the difference between a Pod and a container, a Deployment and a Service, or you know what an Ingress does, well you might skip it.","title":"Audience"},{"location":"#prerequisites","text":"Despite being a 101 course, a solid understanding of Docker fundamentals (containers, images, registry, etc.) is required.","title":"Prerequisites"},{"location":"#setup","text":"Before starting, please install Homebrew and follow these instructions in order to setup your laptop. Some system-wide binaries will be installed with brew while we are going to use asdf-vm for some specific programs like kubectl in order to have a better control over installes versions: Install system-wide required binaries by running brew bundle Install the following asdf-vm plugins by running: asdf plugin-add kubectl asdf plugin-add helm Finally, install versioned dependencies with asdf install","title":"Setup"},{"location":"#kubectl-create-course","text":"You can now move to the first batch of exercises to start the course.","title":"kubectl create course"},{"location":"010-first-contact/","text":"First contact Kubernetes is an orchestrator for deploying containers but you can think about it as an Operating System running your programs. While it's good to know how an Operating System works (and you should keep learning Kubernetes internals beyond this course), as long as you know how to install, upgrade and verify your programs are running fine, that'll be enough. You will always see Kubernetes as a single component but under the hood it runs on a Cluster of machines called Nodes . More nodes means more availability. Bigger nodes means more memory and CPU available to your programs. Every Object managed by Kubernetes is represented by a RESTful resource and we'll see how, at the end of the day, kubectl is just a smart HTTP client. For the scope of this course, we won't need a fully functional multi-node Kubernetes cluster. Instead, we'll be using Kind. KIND stands for Kubernetes IN Docker and it's capable of managing local clusters of one node orchestrating a bunch of Docker containers. At this stage, feel free to ignore the internals and look at Kind for what it is, a tool to start/stop Kubernetes clusters. Another tool you must get familiar with is kubectl , the CLI tool you'll be using to do stuff on any Kubernetes cluster, whatever it's local or a production system. One caveat about kubectl , you need to double check the version you have in your system is compatible with the version of Kubernetes that runs in the cluster: client and server can be at most 1 minor version distant from each other. In our case, the versions are pinned so as long as you've followed the setup instructions you should be good. In this unit we'll start a local Cluster and we'll explore its components with kubectl . Exercise n.1: start the Cluster If kind is correctly installed, all you have to do is: $ kind create cluster --name k8s101 --image kindest/node:v1.18.2 Creating cluster \"kind\" ... \u2713 Ensuring node image (kindest/node:v1.18.2) \ud83d\uddbc \u2713 Preparing nodes \ud83d\udce6 \u2713 Writing configuration \ud83d\udcdc \u2713 Starting control-plane \ud83d\udd79\ufe0f \u2713 Installing CNI \ud83d\udd0c \u2713 Installing StorageClass \ud83d\udcbe Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! \ud83d\udc4b Notice we use a specific image (and not latest) so that we know it'll work with the pinned kubectl version. If you see no errors the default Cluster called kind should be ready to use, and you can confirm it by running: $ kind get clusters kind Exercise n.2: explore the Cluster To get informations about the Cluster, just run kubectl cluster-info To prove that everything is a RESTful resource in Kubernetes, you can increase kubectl 's log verbosity and see how it performs HTTP calls under the hood: kubectl cluster-info -v6 We can query the status of few key components of the Cluster: kubectl get componentstatuses And we can also get the list of all the nodes composing the Cluster: kubectl get nodes The more detailed, more structured version of kubectl get is the command describe , that gives us a comprehensive view of the Objects we query, in this case the nodes of the Cluster: kubectl describe nodes","title":"First contact"},{"location":"010-first-contact/#first-contact","text":"Kubernetes is an orchestrator for deploying containers but you can think about it as an Operating System running your programs. While it's good to know how an Operating System works (and you should keep learning Kubernetes internals beyond this course), as long as you know how to install, upgrade and verify your programs are running fine, that'll be enough. You will always see Kubernetes as a single component but under the hood it runs on a Cluster of machines called Nodes . More nodes means more availability. Bigger nodes means more memory and CPU available to your programs. Every Object managed by Kubernetes is represented by a RESTful resource and we'll see how, at the end of the day, kubectl is just a smart HTTP client. For the scope of this course, we won't need a fully functional multi-node Kubernetes cluster. Instead, we'll be using Kind. KIND stands for Kubernetes IN Docker and it's capable of managing local clusters of one node orchestrating a bunch of Docker containers. At this stage, feel free to ignore the internals and look at Kind for what it is, a tool to start/stop Kubernetes clusters. Another tool you must get familiar with is kubectl , the CLI tool you'll be using to do stuff on any Kubernetes cluster, whatever it's local or a production system. One caveat about kubectl , you need to double check the version you have in your system is compatible with the version of Kubernetes that runs in the cluster: client and server can be at most 1 minor version distant from each other. In our case, the versions are pinned so as long as you've followed the setup instructions you should be good. In this unit we'll start a local Cluster and we'll explore its components with kubectl .","title":"First contact"},{"location":"010-first-contact/#exercise-n1-start-the-cluster","text":"If kind is correctly installed, all you have to do is: $ kind create cluster --name k8s101 --image kindest/node:v1.18.2 Creating cluster \"kind\" ... \u2713 Ensuring node image (kindest/node:v1.18.2) \ud83d\uddbc \u2713 Preparing nodes \ud83d\udce6 \u2713 Writing configuration \ud83d\udcdc \u2713 Starting control-plane \ud83d\udd79\ufe0f \u2713 Installing CNI \ud83d\udd0c \u2713 Installing StorageClass \ud83d\udcbe Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! \ud83d\udc4b Notice we use a specific image (and not latest) so that we know it'll work with the pinned kubectl version. If you see no errors the default Cluster called kind should be ready to use, and you can confirm it by running: $ kind get clusters kind","title":"Exercise n.1: start the Cluster"},{"location":"010-first-contact/#exercise-n2-explore-the-cluster","text":"To get informations about the Cluster, just run kubectl cluster-info To prove that everything is a RESTful resource in Kubernetes, you can increase kubectl 's log verbosity and see how it performs HTTP calls under the hood: kubectl cluster-info -v6 We can query the status of few key components of the Cluster: kubectl get componentstatuses And we can also get the list of all the nodes composing the Cluster: kubectl get nodes The more detailed, more structured version of kubectl get is the command describe , that gives us a comprehensive view of the Objects we query, in this case the nodes of the Cluster: kubectl describe nodes","title":"Exercise n.2: explore the Cluster"},{"location":"020-pods/","text":"Pods A Pod is the minimum deployable unit on kubernetes and consists of one or more Docker containers. You can\u2019t \u201crun a container\u201d in a Kubernetes cluster but you can schedule and \u201crun a pod\u201d. If we keep using the Operating System metaphor, a pod would be a process and its containers would be threads: each container has its own life (for example it has separated cgroups) but at the same time it's strongly coupled with the other containers in the pod by having shared namespaces, same hostname, same IP address and same open ports. It's important to note that a pod is atomic and it always runs on one node of the Cluster. In other words, all the containers in a pod always lands in the same Node of the Cluster. Exercise n.1: create a Pod First of all, create a pod called nginx-pod running a single container and pulling the nginx image from Dockerhub: $ kubectl run nginx-pod --generator=run-pod/v1 --image=nginx Flag --generator has been deprecated, has no effect and will be removed in the future. pod/nginx-pod created Please ignore the deprecation warning for now, this command is only meaningful in the context of this course and you wouldn't use it in a real world scenario. Now to get the list of running pods run: $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-pod 1/1 Running 0 104s As we mentioned in the previous unit, we can get a lot more info about an Object using describe , let's do it with our pod: $ kubectl describe pod nginx-pod Name: nginx-pod Namespace: default ... Finally, we can delete the pod: $ kubectl delete pods nginx-pod pod \"nginx-pod\" deleted Exercise n.2: interact with a Pod Let's create the same pod again but this time we expose its TCP port number 80: kubectl run nginx --generator=run-pod/v1 --image=nginx --port 80 The port is only exposed within the Cluster but there's a way to easily open a connection tunnel to our local host at the port 8080 by running: $ kubectl port-forward nginx-pod 8080:80 Forwarding from 127.0.0.1:8080 -> 80 Forwarding from [::1]:8080 -> 80kubectl port-forward nginx-pod 8080:80 Forwarding from 127.0.0.1:8080 -> 80 Forwarding from [::1]:8080 -> 80 The tunnel will stay open until you hit CTRL+C, in the meantime you can open the browser at http://localhost:8080 and see the pod serving requests. Last but not least, we can also get logs for our pod by running: kubectl logs nginx","title":"Pods"},{"location":"020-pods/#pods","text":"A Pod is the minimum deployable unit on kubernetes and consists of one or more Docker containers. You can\u2019t \u201crun a container\u201d in a Kubernetes cluster but you can schedule and \u201crun a pod\u201d. If we keep using the Operating System metaphor, a pod would be a process and its containers would be threads: each container has its own life (for example it has separated cgroups) but at the same time it's strongly coupled with the other containers in the pod by having shared namespaces, same hostname, same IP address and same open ports. It's important to note that a pod is atomic and it always runs on one node of the Cluster. In other words, all the containers in a pod always lands in the same Node of the Cluster.","title":"Pods"},{"location":"020-pods/#exercise-n1-create-a-pod","text":"First of all, create a pod called nginx-pod running a single container and pulling the nginx image from Dockerhub: $ kubectl run nginx-pod --generator=run-pod/v1 --image=nginx Flag --generator has been deprecated, has no effect and will be removed in the future. pod/nginx-pod created Please ignore the deprecation warning for now, this command is only meaningful in the context of this course and you wouldn't use it in a real world scenario. Now to get the list of running pods run: $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-pod 1/1 Running 0 104s As we mentioned in the previous unit, we can get a lot more info about an Object using describe , let's do it with our pod: $ kubectl describe pod nginx-pod Name: nginx-pod Namespace: default ... Finally, we can delete the pod: $ kubectl delete pods nginx-pod pod \"nginx-pod\" deleted","title":"Exercise n.1: create a Pod"},{"location":"020-pods/#exercise-n2-interact-with-a-pod","text":"Let's create the same pod again but this time we expose its TCP port number 80: kubectl run nginx --generator=run-pod/v1 --image=nginx --port 80 The port is only exposed within the Cluster but there's a way to easily open a connection tunnel to our local host at the port 8080 by running: $ kubectl port-forward nginx-pod 8080:80 Forwarding from 127.0.0.1:8080 -> 80 Forwarding from [::1]:8080 -> 80kubectl port-forward nginx-pod 8080:80 Forwarding from 127.0.0.1:8080 -> 80 Forwarding from [::1]:8080 -> 80 The tunnel will stay open until you hit CTRL+C, in the meantime you can open the browser at http://localhost:8080 and see the pod serving requests. Last but not least, we can also get logs for our pod by running: kubectl logs nginx","title":"Exercise n.2: interact with a Pod"},{"location":"030-declarative/","text":"Going declarative So far we have operated Kubernetes in an imperative fashion, issuing \"orders\" through kubectl but this is not the way Kubernetes was designed to be used. From now on we\u2019ll be using a declarative approach, writing down in YAML syntax how we want the cluster to be after we run kubectl apply . Such Yaml files are called definitions . Exercise n.1: get familiar with kubectl apply Change directory into the folder definitions before running these commands. Apply the definition named two-containers-pod.yaml and see how one of the containers in the pod will create an index.html file that will be then served by the second container in the same pod: $ kubectl apply -f two-containers-pod.yaml pod/two-containers-pod created Then let's see with a browser what the Nginx in that pod is serving: kubectl port-forward two-containers-pod 8080:80 Open the browser at http://localhost:8080. Finally, delete all the pods running in the Cluster: kubectl delete pods --all","title":"Going declarative"},{"location":"030-declarative/#going-declarative","text":"So far we have operated Kubernetes in an imperative fashion, issuing \"orders\" through kubectl but this is not the way Kubernetes was designed to be used. From now on we\u2019ll be using a declarative approach, writing down in YAML syntax how we want the cluster to be after we run kubectl apply . Such Yaml files are called definitions .","title":"Going declarative"},{"location":"030-declarative/#exercise-n1-get-familiar-with-kubectl-apply","text":"Change directory into the folder definitions before running these commands. Apply the definition named two-containers-pod.yaml and see how one of the containers in the pod will create an index.html file that will be then served by the second container in the same pod: $ kubectl apply -f two-containers-pod.yaml pod/two-containers-pod created Then let's see with a browser what the Nginx in that pod is serving: kubectl port-forward two-containers-pod 8080:80 Open the browser at http://localhost:8080. Finally, delete all the pods running in the Cluster: kubectl delete pods --all","title":"Exercise n.1: get familiar with kubectl apply"},{"location":"040-labels/","text":"Labels Labels are simple key/value pairs attached to any Kubernetes object and used at runtime to query and filter such objects. Labels are heavily used by Kubernetes i tself as we\u2019ll see in a moment. Exercise n.1: list labels and use them in queries Change directory into the folder definitions before running these commands. Start two pods with some labels attached (see their definition files for details): kubectl apply -f foo-pod.yaml kubectl apply -f bar-pod.yaml You can see the attached labels for each pod by running: $ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS bar 1/1 Running 0 48s env=staging foo 1/1 Running 0 53s env=prod Now let's see how to list only pods having a label env with prod as value. To use a label in order to filter the output of kubectl get pods , we pass the option --selector to kubectl . You can keep the --show-labels flag to double check the query is working as expected: $ kubectl get pods --selector=\"env=prod\" --show-labels NAME READY STATUS RESTARTS AGE LABELS foo 1/1 Running 0 2m19s env=prod In case you want to see any pod with the label env , whatever its value: kubectl get pods --selector=\"env\" --show-labels Queries can be negated with the ! operator: kubectl get pods --selector=\"\\!env\" --show-labels No resources found in default namespace.","title":"Labels"},{"location":"040-labels/#labels","text":"Labels are simple key/value pairs attached to any Kubernetes object and used at runtime to query and filter such objects. Labels are heavily used by Kubernetes i tself as we\u2019ll see in a moment.","title":"Labels"},{"location":"040-labels/#exercise-n1-list-labels-and-use-them-in-queries","text":"Change directory into the folder definitions before running these commands. Start two pods with some labels attached (see their definition files for details): kubectl apply -f foo-pod.yaml kubectl apply -f bar-pod.yaml You can see the attached labels for each pod by running: $ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS bar 1/1 Running 0 48s env=staging foo 1/1 Running 0 53s env=prod Now let's see how to list only pods having a label env with prod as value. To use a label in order to filter the output of kubectl get pods , we pass the option --selector to kubectl . You can keep the --show-labels flag to double check the query is working as expected: $ kubectl get pods --selector=\"env=prod\" --show-labels NAME READY STATUS RESTARTS AGE LABELS foo 1/1 Running 0 2m19s env=prod In case you want to see any pod with the label env , whatever its value: kubectl get pods --selector=\"env\" --show-labels Queries can be negated with the ! operator: kubectl get pods --selector=\"\\!env\" --show-labels No resources found in default namespace.","title":"Exercise n.1: list labels and use them in queries"},{"location":"050-annotations/","text":"Annotations Annotation s are very similar to labels, as they are key/value pairs, with a big difference: annotations can\u2019t be used to query or filter Kubernetes objects. Annotations are meant to carry on information used by other softwares, like auditing or monitoring. TIP: when in doubt, use annotations and \u201cpromote\u201d them to labels later, if and when you need to use them in a query. Exercise n.1: can't query Assuming the pod called foo from the previous unit is still running, we can attach to it an annotation named version with a value of 1 by running: $ kubectl annotate pods foo version=1 pod/foo annotated Despite the annotation, the filter won't work this time and foo won't be listed: $ kubectl get pods --selector=\"version=1\" No resources found in default namespace. Clean up all the pods before moving to the next step: $ kubectl delete pods --all pod \"bar\" deleted pod \"foo\" deleted","title":"Annotations"},{"location":"050-annotations/#annotations","text":"Annotation s are very similar to labels, as they are key/value pairs, with a big difference: annotations can\u2019t be used to query or filter Kubernetes objects. Annotations are meant to carry on information used by other softwares, like auditing or monitoring. TIP: when in doubt, use annotations and \u201cpromote\u201d them to labels later, if and when you need to use them in a query.","title":"Annotations"},{"location":"050-annotations/#exercise-n1-cant-query","text":"Assuming the pod called foo from the previous unit is still running, we can attach to it an annotation named version with a value of 1 by running: $ kubectl annotate pods foo version=1 pod/foo annotated Despite the annotation, the filter won't work this time and foo won't be listed: $ kubectl get pods --selector=\"version=1\" No resources found in default namespace. Clean up all the pods before moving to the next step: $ kubectl delete pods --all pod \"bar\" deleted pod \"foo\" deleted","title":"Exercise n.1: can't query"},{"location":"060-deployments/","text":"Deployments So far we scheduled our pods manually but this way we don't leverage any of the cool features of Kubernetes, like scaling up and down automatically or self-healing the system when needed. A Deployment is an object that logically groups pods together according to our specifications. An example of specification is the number of desired pods that must run at any given time no matter what. If a pod dies, we want Kubernetes to bring another one up to replace it; if pods are more than we asked for, we want Kubernetes to kill as many as needed to match our request. Exercise n.1: manage a deployment Change directory into the folder definitions before running these commands. Create a Deployment called nginx-prod : $ kubectl apply -f nginx-prod.yaml deployment.apps/nginx-prod created Now let's ask kubectl to list all the objects present in the cluster to see what happened: $ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-prod-d6fd669f4-8tng2 1/1 Running 0 102s pod/nginx-prod-d6fd669f4-9x57t 1/1 Running 0 102s pod/nginx-prod-d6fd669f4-fpxqw 1/1 Running 0 102s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-prod 3/3 3 3 102s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-prod-d6fd669f4 3 3 3 102s As you can see there are now 3 pods, 1 deployment and 1 object we haven't seen yet, a Replicaset . A replicaset has the only responsibility to keep a certain number of pods running at any time; in this case, since our deployment specs defined 3 replicas for our pod, while creating the deployment Kubernetes also attached a replicaset to it. If you run a kubectl describe on the replicaset, you'll notice that it is under the control of our deployment: $ kubectl describe replicaset <the name of the replicaset> Name: nginx-prod-d6fd669f4 Namespace: default <snip> Controlled By: Deployment/nginx-prod <snip> Now let's see the replicaset in action: we're going to kill one pod and see if another one starts to take its place. $ kubectl delete pod <the name of the pod> pod \"the name of the pod\" deleted Respawning a new pod is almost instantaneous, so by the time you run this command again: kubectl get pods you'll see that a new pod with a different name has started, bringing back the pod count to 3.","title":"Deployments"},{"location":"060-deployments/#deployments","text":"So far we scheduled our pods manually but this way we don't leverage any of the cool features of Kubernetes, like scaling up and down automatically or self-healing the system when needed. A Deployment is an object that logically groups pods together according to our specifications. An example of specification is the number of desired pods that must run at any given time no matter what. If a pod dies, we want Kubernetes to bring another one up to replace it; if pods are more than we asked for, we want Kubernetes to kill as many as needed to match our request.","title":"Deployments"},{"location":"060-deployments/#exercise-n1-manage-a-deployment","text":"Change directory into the folder definitions before running these commands. Create a Deployment called nginx-prod : $ kubectl apply -f nginx-prod.yaml deployment.apps/nginx-prod created Now let's ask kubectl to list all the objects present in the cluster to see what happened: $ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-prod-d6fd669f4-8tng2 1/1 Running 0 102s pod/nginx-prod-d6fd669f4-9x57t 1/1 Running 0 102s pod/nginx-prod-d6fd669f4-fpxqw 1/1 Running 0 102s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-prod 3/3 3 3 102s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-prod-d6fd669f4 3 3 3 102s As you can see there are now 3 pods, 1 deployment and 1 object we haven't seen yet, a Replicaset . A replicaset has the only responsibility to keep a certain number of pods running at any time; in this case, since our deployment specs defined 3 replicas for our pod, while creating the deployment Kubernetes also attached a replicaset to it. If you run a kubectl describe on the replicaset, you'll notice that it is under the control of our deployment: $ kubectl describe replicaset <the name of the replicaset> Name: nginx-prod-d6fd669f4 Namespace: default <snip> Controlled By: Deployment/nginx-prod <snip> Now let's see the replicaset in action: we're going to kill one pod and see if another one starts to take its place. $ kubectl delete pod <the name of the pod> pod \"the name of the pod\" deleted Respawning a new pod is almost instantaneous, so by the time you run this command again: kubectl get pods you'll see that a new pod with a different name has started, bringing back the pod count to 3.","title":"Exercise n.1: manage a deployment"}]}